<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/BinEgo-360/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/BinEgo-360/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/BinEgo-360/iccv-hawaii-logo.svg"/><link rel="preload" as="image" href="/BinEgo-360/speakers/addison.png"/><link rel="preload" as="image" href="/BinEgo-360/speakers/dima.png"/><link rel="preload" as="image" href="/BinEgo-360/speakers/bernard.jpg"/><link rel="preload" as="image" href="https://x360dataset.github.io/static/images/overall.gif"/><link rel="preload" as="image" href="/BinEgo-360/organizers/jiao.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/wu.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/campbell.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/wei.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/qi.jpg"/><link rel="stylesheet" href="/BinEgo-360/_next/static/css/fdedd551e0217345.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/BinEgo-360/_next/static/chunks/webpack-3ac31fac96870146.js"/><script src="/BinEgo-360/_next/static/chunks/4bd1b696-67ee12fb04071d3b.js" async=""></script><script src="/BinEgo-360/_next/static/chunks/684-8da6a75fa71dc898.js" async=""></script><script src="/BinEgo-360/_next/static/chunks/main-app-552d8ebf7c822fe1.js" async=""></script><link rel="preload" as="image" href="/BinEgo-360/organizers/placeholder_female.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/leonardis.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/chenyuan.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/han.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/qiming.jpg"/><link rel="preload" as="image" href="/BinEgo-360/organizers/hao.jpg"/><link rel="preload" as="image" href="/BinEgo-360/insta360-logo.png"/><link rel="preload" as="image" href="/BinEgo-360/SCAN-logo.png"/><link rel="preload" as="image" href="/BinEgo-360/allsee-logo.jpg"/><link rel="preload" as="image" href="/BinEgo-360/tencent_logo.png"/><meta name="next-size-adjust" content=""/><title>BinEgo‑360 Challenge</title><meta name="description" content="The BinEgo‑360 Challenge targets human‑like perception by jointly reasoning over 360° panoramic and binocular egocentric video streams, aligned with spatial audio, text and geo‑metadata."/><link rel="icon" href="/BinEgo-360/favicon.ico" type="image/x-icon" sizes="256x256"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/BinEgo-360/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_6cccb2 __variable_a3dd79 antialiased"><header class="fixed inset-x-0 top-0 z-50 bg-white/80 backdrop-blur-md shadow-sm"><div class="flex items-center gap-6 bg-gray-800 px-4 py-1 text-xs text-gray-100"><div class="flex items-center gap-1"><svg class="h-4 w-4" fill="currentColor" viewBox="0 0 24 24"><path d="M12 2a7 7 0 00-7 7c0 5.25 7 13 7 13s7-7.75 7-13a7 7 0 00-7-7zm0 9.5a2.5 2.5 0 112.5-2.5 2.503 2.503 0 01-2.5 2.5z"></path></svg>Room 306 B, Hawaii Convention Center, Honolulu HI, USA</div><div class="flex items-center gap-1"><svg class="h-4 w-4" fill="currentColor" viewBox="0 0 24 24"><path d="M17 12a5 5 0 01-5 5v5h-3v-5a5 5 0 110-10V2h3v5a5 5 0 015 5z"></path></svg>19th Oct, 2025</div><a href="mailto:j.jiao@bham.ac.uk" class="hover:underline">j.jiao@bham.ac.uk</a></div><nav class="mx-auto flex max-w-7xl items-center justify-between px-4 py-3 text-sm font-medium text-gray-700"><div class="flex items-center gap-2 text-base font-semibold"><span class="text-indigo-600">BinEgo‑360°</span><span>Workshop @ ICCV 2025</span></div><ul class="hidden gap-6 md:flex"><li><a href="#home" class="hover:text-indigo-600">Home</a></li><li><a href="#overview" class="hover:text-indigo-600">Overview</a></li><li><a href="#speakers" class="hover:text-indigo-600">Speakers</a></li><li><a href="#programme" class="hover:text-indigo-600">Programme</a></li><li><a href="#invited-papers" class="hover:text-indigo-600">Paper Presentations</a></li><li><a href="#challenge" class="hover:text-indigo-600">Challenge</a></li><li><a href="#organizers" class="hover:text-indigo-600">Organizers</a></li><li><a href="#sponsors" class="hover:text-indigo-600">Sponsors</a></li></ul></nav></header><main id="home" class="pt-24 pb-16"><section class="relative flex items-center justify-center text-center min-h-[540px] bg-cover bg-center bg-no-repeat" style="background-image:url(/BinEgo-360/hawaii-hero.jpg)"><div class="absolute inset-0 bg-black/60"></div><div class="relative z-10 mx-auto max-w-5xl px-4"><img src="/BinEgo-360/iccv-hawaii-logo.svg" alt="ICCV 2025 Honolulu Hawaii" class="mx-auto mb-6 h-20 w-auto"/><h1 class="text-4xl sm:text-5xl font-extrabold tracking-tight text-white">BinEgo‑360°: Binocular Egocentric-360° Multi-modal Scene Understanding in the Wild</h1><p class="mx-auto mt-6 max-w-3xl text-lg text-gray-200">Welcome to the <span class="font-semibold text-indigo-200">BinEgo‑360° Workshop &amp; Challenge</span> at ICCV 2025. We bring together researchers working on<!-- --> <strong>360° panoramic</strong> and <strong>binocular egocentric</strong> vision to explore human‑like perception across <em>video</em>, <em>audio</em>, and <em>geo‑spatial</em> <!-- -->modalities.</p><div class="mt-10 flex flex-wrap justify-center gap-4"><a href="https://media.eventhosts.cc/Conferences/ICCV2025/iccv25_workshops_tutorials.pdf#page=40" target="_blank" rel="noreferrer" class="inline-flex items-center gap-2 rounded-full bg-indigo-600 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-400 focus:ring-offset-2"><svg class="h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 21s-7-4.686-7-11a7 7 0 1 1 14 0c0 6.314-7 11-7 11zm0-9.5a2.5 2.5 0 1 0 0-5 2.5 2.5 0 0 0 0 5z"></path></svg>Venue: Room 306B</a><a href="https://iccv.thecvf.com/virtual/2025/workshop/2749" target="_blank" rel="noreferrer" class="inline-flex items-center gap-2 rounded-full bg-emerald-500 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-emerald-600 focus:outline-none focus:ring-2 focus:ring-emerald-300 focus:ring-offset-2"><svg class="h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 9V5.25A2.25 2.25 0 0 0 13.5 3h-9A2.25 2.25 0 0 0 2.25 5.25v13.5A2.25 2.25 0 0 0 4.5 21h9a2.25 2.25 0 0 0 2.25-2.25V15l6 3.75V5.25L15.75 9z"></path></svg>Join online</a><a href="#programme" class="inline-flex items-center gap-2 rounded-full border border-white/70 bg-white/90 px-6 py-3 text-sm font-semibold text-gray-900 shadow-lg transition hover:bg-white focus:outline-none focus:ring-2 focus:ring-white focus:ring-offset-2"><svg class="h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M8.25 6.75h7.5m-7.5 3h7.5m-7.5 3h4.5M6.75 5.25A2.25 2.25 0 0 0 4.5 7.5v9A2.25 2.25 0 0 0 6.75 18.75h10.5A2.25 2.25 0 0 0 19.5 16.5v-9a2.25 2.25 0 0 0-2.25-2.25H6.75z"></path></svg>View programme</a><a href="#challenge" class="inline-flex items-center gap-2 rounded-full bg-amber-500 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-amber-600 focus:outline-none focus:ring-2 focus:ring-amber-300 focus:ring-offset-2"><svg class="h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M16.5 3.75h2.25A1.25 1.25 0 0 1 20 5v2.25a3.75 3.75 0 0 1-3.75 3.75L16 11.5a4.5 4.5 0 0 1-3.75 3.7V18h2.25a.75.75 0 1 1 0 1.5H9.5A.75.75 0 1 1 9.5 18H11v-2.8A4.5 4.5 0 0 1 7.25 11.5l-.25-.5A3.75 3.75 0 0 1 3.25 7.25V5a1.25 1.25 0 0 1 1.25-1.25H6.75a.75.75 0 0 1 .75.75v4a2.25 2.25 0 1 0 4.5 0v-4a.75.75 0 0 1 .75-.75H15a.75.75 0 0 1 .75.75v4a2.25 2.25 0 1 0 4.5 0v-4a.75.75 0 0 1 .75-.75Z"></path></svg>Participate in challenge</a></div></div></section><section id="overview" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Overview</h2><p class="mt-6 text-gray-700">This half-day workshop mainly looks at multi-modal scene understanding and perception in a human-like way. Specifically, we will focus on <strong>binocular/stereo</strong> egocentric and <strong>360° panoramic</strong> perspectives, which measure both first-person views and third-person panoptic views, mimicking a human in the scene, by combining with multi‑modal cues such as <em>spatial audio</em>, <em>textual descriptions</em>, and<!-- --> <em>geo‑metadata</em>. This workshop will cover but not be limited to the following topics:</p><ul class="mt-4 list-disc space-y-2 pl-6 text-gray-700"><li>Embodied 360° scene understanding &amp; egocentric visual reasoning</li><li>Multi-modal scene understanding</li><li>Stereo Vision</li><li>Open‑world learning &amp; domain adaptation</li></ul></section><section id="speakers" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Keynote Speakers</h2><div class="mt-8 grid gap-8 md:grid-cols-3"><div class="text-center"><a href="https://vlislab22.github.io/vlislab/linwang.html" target="_blank" rel="noreferrer"><img src="/BinEgo-360/speakers/addison.png" alt="Addison Lin Wang" class="mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-lg font-semibold text-gray-900"><a href="https://vlislab22.github.io/vlislab/linwang.html" target="_blank" rel="noreferrer" class="hover:underline">Addison Lin Wang</a></h3><p class="text-sm text-gray-600">Nanyang Technological University</p></div><div class="text-center"><a href="https://dimadamen.github.io/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/speakers/dima.png" alt="Dima Damen" class="mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-lg font-semibold text-gray-900"><a href="https://dimadamen.github.io/" target="_blank" rel="noreferrer" class="hover:underline">Dima Damen</a></h3><p class="text-sm text-gray-600">University of Bristol</p></div><div class="text-center"><a href="https://www.bernardghanem.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/speakers/bernard.jpg" alt="Bernard Ghanem" class="mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-lg font-semibold text-gray-900"><a href="https://www.bernardghanem.com/" target="_blank" rel="noreferrer" class="hover:underline">Bernard Ghanem</a></h3><p class="text-sm text-gray-600">King Abdullah University of Science and Technology</p></div></div></section><section id="programme" class="mx-auto mt-24 max-w-6xl px-4 lg:max-w-7xl"><div class="rounded-3xl bg-white/95 p-8 shadow-2xl ring-1 ring-indigo-100 backdrop-blur-sm"><div class="flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between"><h2 class="text-4xl font-extrabold tracking-tight text-gray-900">Workshop Programme (Half‑day)</h2><span class="inline-flex items-center gap-2 self-start rounded-full bg-indigo-600/10 px-4 py-2 text-sm font-semibold text-indigo-700 ring-1 ring-indigo-200"><svg class="h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M8.25 6.75h7.5M8.25 10.5h7.5m-7.5 3.75h4.5m7.5-1.5A2.25 2.25 0 0 1 19.5 18v1.5A2.25 2.25 0 0 1 17.25 21H6.75A2.25 2.25 0 0 1 4.5 18.75V18a2.25 2.25 0 0 1 2.25-2.25h12.75Zm0-9.75V6A2.25 2.25 0 0 1 19.5 8.25H4.5A2.25 2.25 0 0 1 2.25 6V4.5A2.25 2.25 0 0 1 4.5 2.25h15A2.25 2.25 0 0 1 21.75 4.5V6Z"></path></svg>Room 306B · 19 Oct 2025</span></div><div class="mt-6 overflow-x-auto"><table class="w-full min-w-[640px] text-left text-base text-gray-900"><thead class="bg-indigo-600/90 text-white"><tr><th class="whitespace-nowrap px-6 py-3 text-lg font-semibold tracking-wide">Time</th><th class="px-6 py-3 text-lg font-semibold tracking-wide">Session</th></tr></thead><tbody class="divide-y divide-indigo-100"><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">09:00 – 09:30</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="font-semibold text-gray-900">Opening Remarks</span></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">09:30 – 10:05</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800"><span class="rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700">Keynote</span>Bernard Ghanem — Towards Robust Multimodal Egocentric Video Understanding</span></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">10:05 – 10:40</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800"><span class="rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700">Keynote</span>Dima Damen — Video Understanding Out of the Frame: An Egocentric Perspective</span></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">10:40 – 11:00</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="font-semibold text-gray-900">Break &amp; Poster Session</span></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">11:00 – 11:45</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><div class="space-y-4 text-sm text-gray-700 md:text-base"><span class="rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700">Invited Paper Presentations</span><ul class="space-y-1 leading-relaxed"><li class="space-y-1"><a href="https://red-fairy.github.io/argus/" target="_blank" rel="noopener noreferrer" class="font-semibold text-amber-700 hover:text-amber-800 hover:underline">Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos</a><div class="text-sm italic text-gray-600 md:text-base">Presenter: Rundong Luo</div></li><li class="space-y-1"><a href="https://schowdhury671.github.io/egoadapt_project/" target="_blank" rel="noopener noreferrer" class="font-semibold text-amber-700 hover:text-amber-800 hover:underline">EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception</a><div class="text-sm italic text-gray-600 md:text-base">Presenter: Sanjoy Chowdhury</div></li><li class="space-y-1"><a href="https://vision.cs.utexas.edu/projects/switch_a_view/" target="_blank" rel="noopener noreferrer" class="font-semibold text-amber-700 hover:text-amber-800 hover:underline">Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos</a><div class="text-sm italic text-gray-600 md:text-base">Presenter: Sagnik Majumder</div></li></ul></div></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">11:45 – 12:20</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800"><span class="rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700">Keynote</span>Addison Lin Wang — 360 Vision in the Foundation AI Era: Principles, Methods, and Future Directions</span></div></td></tr><tr class="odd:bg-white even:bg-indigo-50/40 align-top"><td class="whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700">12:20 – 12:35</td><td class="px-6 py-4 align-top text-base text-gray-900"><div class="space-y-4"><span class="font-semibold text-gray-900">Awards Ceremony &amp; Concluding Remarks</span></div></td></tr></tbody></table></div></div></section><section id="challenge" class="mx-auto mt-24 max-w-6xl px-4"><h2 class="text-3xl font-bold text-gray-900">BinEgo‑360° Challenge</h2><p class="mt-6 text-gray-700">The challenge uses our public dataset <a href="https://x360dataset.github.io/" target="_blank" rel="noreferrer" class="underline">360+x</a> for training/validation, and a held-out test set for the evaluation. <ul>Apart from the prizes, winners of the challenge will be invited to submit a paper/report to be included in the workshop proceedings and present at the workshop.</ul> For more details about the dataset, tracks, timeline, and submission rules, please see below:</p><div class="mt-6 rounded border-l-4 border-yellow-400 bg-yellow-50 p-4 text-gray-700"><p class="font-semibold">Important Notice:</p><p class="mt-2"><b>New Final Submission Deadline  ▶  20 August 2025 (AOE)</b></p><p class="mt-2">We are delighted to let you know that the challenge deadline has been extended! Now you have more time to cook your good stuff!</p></div></section><section id="dataset" class="mx-auto mt-24 max-w-6xl px-4"><h2 class="text-3xl font-bold text-gray-900">Dataset Overview</h2><div class="mt-8 grid gap-8 md:grid-cols-2"><div class="w-full h-[400px] overflow-hidden rounded-lg shadow-md"><img src="https://x360dataset.github.io/static/images/overall.gif" alt="Dataset montage" class="w-full h-full object-cover"/></div><div class="flex flex-col justify-center text-gray-700"><ul class="space-y-3"><li>2,152 videos – 8.579 M frames / 67.78 h.</li><li><strong>Viewpoints</strong>: 360° panoramic, binocular &amp; monocular egocentric, third‑person front.</li><li><strong>Modalities</strong>: RGB video, 6‑channel spatial audio, GPS + weather, text scene description.</li><li><strong>Annotations</strong>: 38 action classes, temporal segments; object bounding boxes.</li><li><strong>Resolution</strong>: 5 K originals (5 760 × 2 880 pano).</li><li><strong>License</strong>: CC BY‑NC‑SA 4.0. All faces auto‑blurred.</li></ul><div class="mt-6 flex flex-wrap gap-4"><a href="https://huggingface.co/datasets/quchenyuan/360x_dataset_HR" class="rounded bg-gray-800 px-4 py-2 text-sm font-medium text-white hover:bg-gray-900">Download HR</a><a href="https://huggingface.co/datasets/quchenyuan/360x_dataset_LR" class="rounded border border-gray-300 bg-white px-4 py-2 text-sm font-medium text-gray-700 hover:border-gray-400">Download LR</a><a href="https://arxiv.org/abs/2404.00989" class="rounded border border-indigo-600 bg-indigo-50 px-4 py-2 text-sm font-medium text-indigo-700 hover:bg-indigo-100">Paper (CVPR 2024)</a><a href="https://github.com/x360dataset/x360dataset-kit" class="rounded border border-gray-300 bg-white px-4 py-2 text-sm font-medium text-gray-700 hover:border-gray-400">Baseline Code</a></div></div></div></section><section id="tracks" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Challenge Tracks &amp; Baselines</h2><div class="mt-10 grid gap-20 md:grid-cols-2"><div class="flex flex-col justify-between rounded-xl border border-gray-200 bg-white p-6 shadow-sm"><div><h3 class="text-xl font-semibold text-indigo-600">1 · Classification</h3><p class="mt-2 text-gray-600">Predict the scene label for a whole clip. We follow the scene categories provided in the dataset.</p><ul class="mt-4 list-disc space-y-1 pl-5 text-sm text-gray-700"><li><strong>Input</strong>: 360° RGB + egocentric RGB + audio/binaural delay.</li><li><strong>Output</strong>: The scene label.</li><li><strong>Metric</strong>: Top‑1 Accuracy (in test set).</li></ul></div><div class="mt-6 rounded bg-gray-50 p-4 text-sm"><p class="font-medium">Baseline (All views and modalities use)</p><p>Top‑1 Acc: <span class="font-semibold">80.62  %</span></p></div><div class="mt-8 text-center"><a href="https://www.kaggle.com/competitions/bin-ego-360-challenge-classification-ext" target="_blank" rel="noreferrer" class="inline-flex items-center gap-2 rounded-lg bg-sky-500 px-6 py-3 text-white shadow hover:bg-sky-600 transition"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="h-5 w-5" fill="currentColor"><path d="M4 4h4v7.586l8.293-8.293 2.414 2.414L10.414 14l8.293 8.293-2.414 2.414L8 16.414V24H4V4z"></path></svg>Join on Kaggle</a></div></div><div class="flex flex-col justify-between rounded-xl border border-gray-200 bg-white p-6 shadow-sm"><div><h3 class="text-xl font-semibold text-indigo-600">2 · Temporal Action Localization</h3><p class="mt-2 text-gray-600">Detect the start and end time of every action instance inside a clip.</p><ul class="mt-4 list-disc space-y-1 pl-5 text-sm text-gray-700"><li><strong>Input</strong>: Same modalities as Track 1</li><li><strong>Output</strong>: JSON output for each detection:<code>{&quot;video_id&quot;: ..., &quot;t_start&quot;: ..., &quot;t_end&quot;: ..., &quot;label&quot;: ...}</code></li><li><strong>Metric</strong>: mAP averaged over IoU ∈ <!-- -->{0.5, 0.75, 0.95}<!-- -->.</li></ul></div><div class="mt-6 rounded bg-gray-50 p-4 text-sm"><p class="font-medium mb-2">Baseline (TriDet + VAD)</p><table class="w-full text-sm text-left text-gray-700 border border-gray-200 rounded"><thead class="bg-gray-100"><tr><th class="px-3 py-2 border-b">Metric</th><th class="px-3 py-2 border-b">Score</th></tr></thead><tbody><tr><td class="px-3 py-2 border-b">mAP@0.5</td><td class="px-3 py-2 border-b font-semibold">27.1</td></tr><tr><td class="px-3 py-2 border-b">mAP@0.75</td><td class="px-3 py-2 border-b font-semibold">18.7</td></tr><tr><td class="px-3 py-2 border-b">mAP@0.95</td><td class="px-3 py-2 border-b font-semibold">7.0</td></tr><tr><td class="px-3 py-2">Average</td><td class="px-3 py-2 font-semibold">17.6</td></tr></tbody></table></div><div class="mt-8 text-center"><a href="https://www.kaggle.com/competitions/bin-ego-360-challenge-tal-ext" target="_blank" rel="noreferrer" class="inline-flex items-center gap-2 rounded-lg bg-sky-500 px-6 py-3 text-white shadow hover:bg-sky-600 transition"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="h-5 w-5" fill="currentColor"><path d="M4 4h4v7.586l8.293-8.293 2.414 2.414L10.414 14l8.293 8.293-2.414 2.414L8 16.414V24H4V4z"></path></svg>Join on Kaggle</a></div></div></div></section><section id="timeline" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Timeline (Anywhere on Earth)</h2><ol class="mt-8 border-l-2 border-indigo-600"><li class="relative ml-6 pb-8 last:pb-0"><span class="absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600"></span><span class="font-medium text-gray-900">1 Jun 2025</span><span class="ml-2 text-gray-600">Dataset &amp; baselines release; Kaggle opens</span></li><li class="relative ml-6 pb-8 last:pb-0"><span class="absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600"></span><span class="font-medium text-gray-900">20 Aug 2025</span><span class="ml-2 text-gray-600">Submission deadline</span></li><li class="relative ml-6 pb-8 last:pb-0"><span class="absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600"></span><span class="font-medium text-gray-900">Sep 2025</span><span class="ml-2 text-gray-600">Technical report and poster due</span></li><li class="relative ml-6 pb-8 last:pb-0"><span class="absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600"></span><span class="font-medium text-gray-900">19-20 Oct 2025</span><span class="ml-2 text-gray-600">Awards &amp; talks at ICCV 2025 workshop</span></li></ol></section><section id="submission" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Submission Rules</h2><ol class="mt-6 list-decimal space-y-3 pl-6 text-gray-700"><li>Teams (≤  5 members) register on Kaggle and fill in the team form.</li><li>Up to <strong>5 submissions per track per team</strong> – the last one counts.</li><li>The winners need to submit a technical report and a poster to be presented at the workshop</li><li>No external data that overlaps with the hidden test clips.</li><li>Any submission after the deadline will not be considered.</li></ol></section><section id="prizes" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Prizes &amp; Sponsors</h2><ul class="mt-6 space-y-2 text-gray-700"><li><strong>Hardware:</strong> Insta360 X5 panoramic camera (<a href="https://www.insta360.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Insta360</a>)</li><li><strong>Compute:</strong> GPU Cloud Credit (worth £5-15k) (<a href="https://www.scan.co.uk/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">SCAN</a>)</li><li><strong>Gift:</strong> Amazon / Taobao vouchers (<a href="https://www.allsee-tech.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Allsee</a>)</li><li><strong>Registration:</strong> Workshop fee for the 1st &amp; 2nd winners on each track (<a href="https://www.tencent.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Tencent</a>)</li></ul><p class="mt-4 text-gray-700">Sponsored by <a href="https://www.insta360.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Insta360</a> · <a href="https://www.scan.co.uk/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">SCAN</a> · <a href="https://www.allsee-tech.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Allsee</a> · <a href="https://www.tencent.com/" target="_blank" rel="noreferrer" class="text-indigo-600 hover:underline">Tencent</a></p></section><section class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Ethics &amp; Broader Impact</h2><p class="mt-4 text-gray-700">All videos were recorded in public or non‑sensitive areas with informed participant consent. Faces are automatically blurred, and the dataset is released for non‑commercial research under CC BY‑NC‑SA 4.0. We prohibit any re‑identification, surveillance or commercial use. By advancing robust multi‑modal perception, we aim to benefit robotics, AR/VR and assistive tech while upholding fairness and privacy.</p></section><section id="organizers" class="mx-auto mt-24 max-w-4xl px-4"><h2 class="text-3xl font-bold text-gray-900">Organisers</h2><div class="mt-8 grid gap-8 md:grid-cols-3 lg:grid-cols-4"><div class="text-center"><a href="https://jianbojiao.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/jiao.jpg" alt="Jianbo Jiao" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://jianbojiao.com/" target="_blank" rel="noreferrer" class="hover:underline">Jianbo Jiao</a></h3><p class="text-sm text-gray-600">University of Birmingham</p></div><div class="text-center"><a href="https://elliottwu.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/wu.jpg" alt="Shangzhe Wu" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://elliottwu.com/" target="_blank" rel="noreferrer" class="hover:underline">Shangzhe Wu</a></h3><p class="text-sm text-gray-600">University of Cambridge</p></div><div class="text-center"><a href="https://sites.google.com/view/djcampbell" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/campbell.jpg" alt="Dylan Campbell" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://sites.google.com/view/djcampbell" target="_blank" rel="noreferrer" class="hover:underline">Dylan Campbell</a></h3><p class="text-sm text-gray-600">Australian National University</p></div><div class="text-center"><a href="https://weiyc.github.io/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/wei.jpg" alt="Yunchao Wei" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://weiyc.github.io/" target="_blank" rel="noreferrer" class="hover:underline">Yunchao Wei</a></h3><p class="text-sm text-gray-600">Beijing Jiaotong University</p></div><div class="text-center"><a href="http://luqi.info/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/qi.jpg" alt="Lu Qi" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="http://luqi.info/" target="_blank" rel="noreferrer" class="hover:underline">Lu Qi</a></h3><p class="text-sm text-gray-600">Insta360</p></div><div class="text-center"><a href="https://www.audioscenic.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/placeholder_female.jpg" alt="Yasmine Mellah" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://www.audioscenic.com/" target="_blank" rel="noreferrer" class="hover:underline">Yasmine Mellah</a></h3><p class="text-sm text-gray-600">Audioscenic</p></div><div class="text-center"><a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/leonardis-ales" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/leonardis.jpg" alt="Aleš Leonardis" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/leonardis-ales" target="_blank" rel="noreferrer" class="hover:underline">Aleš Leonardis</a></h3><p class="text-sm text-gray-600">University of Birmingham</p></div></div><p class="mt-8 text-gray-700"><span class="font-semibold">Technical Committee:</span></p><div class="mt-8 grid gap-8 md:grid-cols-3 lg:grid-cols-4"><div class="text-center"><a href="https://chenyuanqu.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/chenyuan.jpg" alt="Chenyuan Qu" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://chenyuanqu.com/" target="_blank" rel="noreferrer" class="hover:underline">Chenyuan Qu</a></h3><p class="text-sm text-gray-600">University of Birmingham</p></div><div class="text-center"><a href="https://scholar.google.com/Publicationss?hl=zh-CN&amp;view_op=list_works&amp;gmla=ALUCkoXDRY1FyBSlDC4q0bpK9zpnxnhaf2PzJqv2dgVESTCALg71TCdFa7PGpFqiTrWvhnZalzAY234KBYkLCs4O7U4&amp;user=UJRtTJ0AAAAJ" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/han.jpg" alt="Han Hu" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://scholar.google.com/Publicationss?hl=zh-CN&amp;view_op=list_works&amp;gmla=ALUCkoXDRY1FyBSlDC4q0bpK9zpnxnhaf2PzJqv2dgVESTCALg71TCdFa7PGpFqiTrWvhnZalzAY234KBYkLCs4O7U4&amp;user=UJRtTJ0AAAAJ" target="_blank" rel="noreferrer" class="hover:underline">Han Hu</a></h3><p class="text-sm text-gray-600">University of Birmingham</p></div><div class="text-center"><a href="https://qiming-huang.github.io/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/qiming.jpg" alt="Qiming Huang" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://qiming-huang.github.io/" target="_blank" rel="noreferrer" class="hover:underline">Qiming Huang</a></h3><p class="text-sm text-gray-600">University of Birmingham</p></div><div class="text-center"><a href="https://h-chen.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/organizers/hao.jpg" alt="Hao Chen" class="mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg"/></a><h3 class="mt-3 text-base font-semibold text-gray-900"><a href="https://h-chen.com/" target="_blank" rel="noreferrer" class="hover:underline">Hao Chen</a></h3><p class="text-sm text-gray-600">University of Cambridge</p></div></div><p class="mt-4 text-gray-700">Contact: <a href="mailto:j.jiao@bham.ac.uk" class="text-indigo-600 hover:underline">j.jiao@bham.ac.uk</a></p></section><section id="sponsors" class="mx-auto mt-24 max-w-10xl px-4 text-center"><h2 class="text-3xl font-bold text-gray-900">Sponsors</h2><p class="mt-6 text-gray-700">We gratefully acknowledge the generous support of our sponsors.</p><div class="mt-10 flex flex-wrap justify-center gap-10"><a href="https://www.insta360.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/insta360-logo.png" alt="Insta360" class="h-24 object-contain"/></a><a href="https://www.scan.co.uk/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/SCAN-logo.png" alt="SCAN" class="h-24 object-contain"/></a><a href="https://www.allsee-tech.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/allsee-logo.jpg" alt="Allsee" class="h-24 object-contain"/></a><a href="https://www.tencent.com/" target="_blank" rel="noreferrer"><img src="/BinEgo-360/tencent_logo.png" alt="Tencent" class="h-24 object-contain"/></a></div></section><section class="mx-auto mt-24 max-w-3xl px-4"><h2 class="text-3xl font-bold text-gray-900">Publication(s)</h2><p class="mt-4 text-gray-700">If you use the 360+x dataset or participate in the challenge, please consider cite:</p><pre class="mt-4 rounded bg-gray-100 p-4 text-sm leading-tight text-gray-800 overflow-x-auto">@inproceedings{chen2024x360,
  title     = {360+x: A Panoptic Multi-modal Scene Understanding Dataset},
  author    = {Chen, Hao and Hou, Yuqi and Qu, Chenyuan and Testini, Irene and Hong, Xiaohan and Jiao, Jianbo},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2024}
}</pre></section><footer class="mt-32 bg-gray-50 py-6 text-center text-sm text-gray-600"><p>© <!-- -->2025<!-- --> BinEgo‑360° Workshop. Built with Next.js &amp; Tailwind CSS. Hosted on GitHub Pages.</p></footer></main><!--$--><!--/$--><!--$--><!--/$--><script src="/BinEgo-360/_next/static/chunks/webpack-3ac31fac96870146.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9665,[],\"MetadataBoundary\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[4911,[],\"AsyncMetadataOutlet\"]\nb:I[9665,[],\"ViewportBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/BinEgo-360/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/BinEgo-360/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/BinEgo-360/_next/static/css/fdedd551e0217345.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"VKNFTk_Vt3yH05LE1J392\",\"p\":\"/BinEgo-360\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/BinEgo-360/_next/static/css/fdedd551e0217345.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_6cccb2 __variable_a3dd79 antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[null,[\"$\",\"header\",null,{\"className\":\"fixed inset-x-0 top-0 z-50 bg-white/80 backdrop-blur-md shadow-sm\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-6 bg-gray-800 px-4 py-1 text-xs text-gray-100\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"d\":\"M12 2a7 7 0 00-7 7c0 5.25 7 13 7 13s7-7.75 7-13a7 7 0 00-7-7zm0 9.5a2.5 2.5 0 112.5-2.5 2.503 2.503 0 01-2.5 2.5z\"}]}],\"Room 306 B, Hawaii Convention Center, Honolulu HI, USA\"]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"d\":\"M17 12a5 5 0 01-5 5v5h-3v-5a5 5 0 110-10V2h3v5a5 5 0 015 5z\"}]}],\"19th Oct, 2025\"]}],[\"$\",\"a\",null,{\"href\":\"mailto:j.jiao@bham.ac.uk\",\"className\":\"hover:underline\",\"children\":\"j.jiao@bham.ac.uk\"}]]}],[\"$\",\"nav\",null,{\"className\":\"mx-auto flex max-w-7xl items-center justify-between px-4 py-3 text-sm font-medium text-gray-700\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2 text-base font-semibold\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-indigo-600\",\"children\":\"BinEgo‑360°\"}],[\"$\",\"span\",null,{\"children\":\"Workshop @ ICCV 2025\"}]]}],[\"$\",\"ul\",null,{\"className\":\"hidden gap-6 md:flex\",\"children\":[[\"$\",\"li\",\"#home\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#home\",\"className\":\"hover:text-indigo-600\",\"children\":\"Home\"}]}],[\"$\",\"li\",\"#overview\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#overview\",\"className\":\"hover:text-indigo-600\",\"children\":\"Overview\"}]}],[\"$\",\"li\",\"#speakers\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#speakers\",\"className\":\"hover:text-indigo-600\",\"children\":\"Speakers\"}]}],[\"$\",\"li\",\"#programme\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#programme\",\"className\":\"hover:text-indigo-600\",\"children\":\"Programme\"}]}],[\"$\",\"li\",\"#invited-papers\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#invited-papers\",\"className\":\"hover:text-indigo-600\",\"children\":\"Paper Presentations\"}]}],[\"$\",\"li\",\"#challenge\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#challenge\",\"className\":\"hover:text-indigo-600\",\"children\":\"Challenge\"}]}],[\"$\",\"li\",\"#organizers\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#organizers\",\"className\":\"hover:text-indigo-600\",\"children\":\"Organizers\"}]}],[\"$\",\"li\",\"#sponsors\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#sponsors\",\"className\":\"hover:text-indigo-600\",\"children\":\"Sponsors\"}]}]]}]]}]]}],[\"$\",\"main\",null,{\"id\":\"home\",\"className\":\"pt-24 pb-16\",\"children\":[[\"$\",\"section\",null,{\"className\":\"relative flex items-center justify-center text-center min-h-[540px] bg-cover bg-center bg-no-repeat\",\"style\":{\"backgroundImage\":\"url(/BinEgo-360/hawaii-hero.jpg)\"},\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute inset-0 bg-black/60\"}],[\"$\",\"div\",null,{\"className\":\"relative z-10 mx-auto max-w-5xl px-4\",\"children\":[[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/iccv-hawaii-logo.svg\",\"alt\":\"ICCV 2025 Honolulu Hawaii\",\"className\":\"mx-auto mb-6 h-20 w-auto\"}],[\"$\",\"h1\",null,{\"className\":\"text-4xl sm:text-5xl font-extrabold tracking-tight text-white\",\"children\":\"BinEgo‑360°: Binocular Egocentric-360° Multi-modal Scene Understanding in the Wild\"}],[\"$\",\"p\",null,{\"className\":\"mx-auto mt-6 max-w-3xl text-lg text-gray-200\",\"children\":[\"Welcome to the \",[\"$\",\"span\",null,{\"className\":\"font-semibold text-indigo-200\",\"children\":\"BinEgo‑360° Workshop \u0026 Challenge\"}],\" at ICCV 2025. We bring together researchers working on\",\" \",[\"$\",\"strong\",null,{\"children\":\"360° panoramic\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"binocular egocentric\"}],\" vision to explore human‑like perception across \",[\"$\",\"em\",null,{\"children\":\"video\"}],\", \",[\"$\",\"em\",null,{\"children\":\"audio\"}],\", and \",[\"$\",\"em\",null,{\"children\":\"geo‑spatial\"}],\" \",\"modalities.\"]}],[\"$\",\"div\",null,{\"className\":\"mt-10 flex flex-wrap justify-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://media.eventhosts.cc/Conferences/ICCV2025/iccv25_workshops_tutorials.pdf#page=40\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"inline-flex items-center gap-2 rounded-full bg-indigo-600 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-400 focus:ring-offset-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"strokeWidth\":\"1.5\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M12 21s-7-4.686-7-11a7 7 0 1 1 14 0c0 6.314-7 11-7 11zm0-9.5a2.5 2.5 0 1 0 0-5 2.5 2.5 0 0 0 0 5z\"}]}],\"Venue: Room 306B\"]}],[\"$\",\"a\",null,{\"href\":\"https://iccv.thecvf.com/virtual/2025/workshop/2749\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"inline-flex items-center gap-2 rounded-full bg-emerald-500 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-emerald-600 focus:outline-none focus:ring-2 focus:ring-emerald-300 focus:ring-offset-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"strokeWidth\":\"1.5\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M15.75 9V5.25A2.25 2.25 0 0 0 13.5 3h-9A2.25 2.25 0 0 0 2.25 5.25v13.5A2.25 2.25 0 0 0 4.5 21h9a2.25 2.25 0 0 0 2.25-2.25V15l6 3.75V5.25L15.75 9z\"}]}],\"Join online\"]}],[\"$\",\"a\",null,{\"href\":\"#programme\",\"className\":\"inline-flex items-center gap-2 rounded-full border border-white/70 bg-white/90 px-6 py-3 text-sm font-semibold text-gray-900 shadow-lg transition hover:bg-white focus:outline-none focus:ring-2 focus:ring-white focus:ring-offset-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"strokeWidth\":\"1.5\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M8.25 6.75h7.5m-7.5 3h7.5m-7.5 3h4.5M6.75 5.25A2.25 2.25 0 0 0 4.5 7.5v9A2.25 2.25 0 0 0 6.75 18.75h10.5A2.25 2.25 0 0 0 19.5 16.5v-9a2.25 2.25 0 0 0-2.25-2.25H6.75z\"}]}],\"View programme\"]}],[\"$\",\"a\",null,{\"href\":\"#challenge\",\"className\":\"inline-flex items-center gap-2 rounded-full bg-amber-500 px-6 py-3 text-sm font-semibold text-white shadow-lg transition hover:bg-amber-600 focus:outline-none focus:ring-2 focus:ring-amber-300 focus:ring-offset-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"strokeWidth\":\"1.5\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M16.5 3.75h2.25A1.25 1.25 0 0 1 20 5v2.25a3.75 3.75 0 0 1-3.75 3.75L16 11.5a4.5 4.5 0 0 1-3.75 3.7V18h2.25a.75.75 0 1 1 0 1.5H9.5A.75.75 0 1 1 9.5 18H11v-2.8A4.5 4.5 0 0 1 7.25 11.5l-.25-.5A3.75 3.75 0 0 1 3.25 7.25V5a1.25 1.25 0 0 1 1.25-1.25H6.75a.75.75 0 0 1 .75.75v4a2.25 2.25 0 1 0 4.5 0v-4a.75.75 0 0 1 .75-.75H15a.75.75 0 0 1 .75.75v4a2.25 2.25 0 1 0 4.5 0v-4a.75.75 0 0 1 .75-.75Z\"}]}],\"Participate in challenge\"]}]]}]]}]]}],[\"$\",\"section\",null,{\"id\":\"overview\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Overview\"}],[\"$\",\"p\",null,{\"className\":\"mt-6 text-gray-700\",\"children\":[\"This half-day workshop mainly looks at multi-modal scene understanding and perception in a human-like way. Specifically, we will focus on \",[\"$\",\"strong\",null,{\"children\":\"binocular/stereo\"}],\" egocentric and \",[\"$\",\"strong\",null,{\"children\":\"360° panoramic\"}],\" perspectives, which measure both first-person views and third-person panoptic views, mimicking a human in the scene, by combining with multi‑modal cues such as \",[\"$\",\"em\",null,{\"children\":\"spatial audio\"}],\", \",[\"$\",\"em\",null,{\"children\":\"textual descriptions\"}],\", and\",\" \",[\"$\",\"em\",null,{\"children\":\"geo‑metadata\"}],\". This workshop will cover but not be limited to the following topics:\"]}],[\"$\",\"ul\",null,{\"className\":\"mt-4 list-disc space-y-2 pl-6 text-gray-700\",\"children\":[[\"$\",\"li\",null,{\"children\":\"Embodied 360° scene understanding \u0026 egocentric visual reasoning\"}],[\"$\",\"li\",null,{\"children\":\"Multi-modal scene understanding\"}],[\"$\",\"li\",null,{\"children\":\"Stereo Vision\"}],[\"$\",\"li\",null,{\"children\":\"Open‑world learning \u0026 domain adaptation\"}]]}]]}],[\"$\",\"section\",null,{\"id\":\"speakers\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Keynote Speakers\"}],[\"$\",\"div\",null,{\"className\":\"mt-8 grid gap-8 md:grid-cols-3\",\"children\":[[\"$\",\"div\",\"Addison Lin Wang\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://vlislab22.github.io/vlislab/linwang.html\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/speakers/addison.png\",\"alt\":\"Addison Lin Wang\",\"className\":\"mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-lg font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://vlislab22.github.io/vlislab/linwang.html\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Addison Lin Wang\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"Nanyang Technological University\"}]]}],[\"$\",\"div\",\"Dima Damen\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://dimadamen.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/speakers/dima.png\",\"alt\":\"Dima Damen\",\"className\":\"mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-lg font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://dimadamen.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Dima Damen\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Bristol\"}]]}],[\"$\",\"div\",\"Bernard Ghanem\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.bernardghanem.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/speakers/bernard.jpg\",\"alt\":\"Bernard Ghanem\",\"className\":\"mx-auto h-36 w-36 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-lg font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.bernardghanem.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Bernard Ghanem\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"King Abdullah University of Science and Technology\"}]]}]]}]]}],[\"$\",\"section\",null,{\"id\":\"programme\",\"className\":\"mx-auto mt-24 max-w-6xl px-4 lg:max-w-7xl\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-3xl bg-white/95 p-8 shadow-2xl ring-1 ring-indigo-100 backdrop-blur-sm\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-4xl font-extrabold tracking-tight text-gray-900\",\"children\":\"Workshop Programme (Half‑day)\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex items-center gap-2 self-start rounded-full bg-indigo-600/10 px-4 py-2 text-sm font-semibold text-indigo-700 ring-1 ring-indigo-200\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"h-4 w-4\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"stroke\":\"currentColor\",\"strokeWidth\":\"1.5\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M8.25 6.75h7.5M8.25 10.5h7.5m-7.5 3.75h4.5m7.5-1.5A2.25 2.25 0 0 1 19.5 18v1.5A2.25 2.25 0 0 1 17.25 21H6.75A2.25 2.25 0 0 1 4.5 18.75V18a2.25 2.25 0 0 1 2.25-2.25h12.75Zm0-9.75V6A2.25 2.25 0 0 1 19.5 8.25H4.5A2.25 2.25 0 0 1 2.25 6V4.5A2.25 2.25 0 0 1 4.5 2.25h15A2.25 2.25 0 0 1 21.75 4.5V6Z\"}]}],\"Room 306B · 19 Oct 2025\"]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-6 overflow-x-auto\",\"children\":[\"$\",\"table\",null,{\"className\":\"w-full min-w-[640px] text-left text-base text-gray-900\",\"children\":[[\"$\",\"thead\",null,{\"className\":\"bg-indigo-600/90 text-white\",\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"className\":\"whitespace-nowrap px-6 py-3 text-lg font-semibold tracking-wide\",\"children\":\"Time\"}],[\"$\",\"th\",null,{\"className\":\"px-6 py-3 text-lg font-semibold tracking-wide\",\"children\":\"Session\"}]]}]}],[\"$\",\"tbody\",null,{\"className\":\"divide-y divide-indigo-100\",\"children\":[[\"$\",\"tr\",\"0\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"09:00 – 09:30\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold text-gray-900\",\"children\":\"Opening Remarks\"}],false]}]}]]}],[\"$\",\"tr\",\"1\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"09:30 – 10:05\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800\",\"children\":[[\"$\",\"span\",null,{\"className\":\"rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700\",\"children\":\"Keynote\"}],\"Bernard Ghanem — Towards Robust Multimodal Egocentric Video Understanding\"]}],false]}]}]]}],[\"$\",\"tr\",\"2\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"10:05 – 10:40\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800\",\"children\":[[\"$\",\"span\",null,{\"className\":\"rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700\",\"children\":\"Keynote\"}],\"Dima Damen — Video Understanding Out of the Frame: An Egocentric Perspective\"]}],false]}]}]]}],[\"$\",\"tr\",\"3\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"10:40 – 11:00\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold text-gray-900\",\"children\":\"Break \u0026 Poster Session\"}],false]}]}]]}],[\"$\",\"tr\",\"4\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"11:00 – 11:45\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"space-y-4 text-sm text-gray-700 md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700\",\"children\":\"Invited Paper Presentations\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 leading-relaxed\",\"children\":[[\"$\",\"li\",\"0\",{\"className\":\"space-y-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://red-fairy.github.io/argus/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"font-semibold text-amber-700 hover:text-amber-800 hover:underline\",\"children\":\"Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos\"}],[\"$\",\"div\",null,{\"className\":\"text-sm italic text-gray-600 md:text-base\",\"children\":\"Presenter: Rundong Luo\"}]]}],[\"$\",\"li\",\"1\",{\"className\":\"space-y-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://schowdhury671.github.io/egoadapt_project/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"font-semibold text-amber-700 hover:text-amber-800 hover:underline\",\"children\":\"EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception\"}],[\"$\",\"div\",null,{\"className\":\"text-sm italic text-gray-600 md:text-base\",\"children\":\"Presenter: Sanjoy Chowdhury\"}]]}],[\"$\",\"li\",\"2\",{\"className\":\"space-y-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://vision.cs.utexas.edu/projects/switch_a_view/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"font-semibold text-amber-700 hover:text-amber-800 hover:underline\",\"children\":\"Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos\"}],[\"$\",\"div\",null,{\"className\":\"text-sm italic text-gray-600 md:text-base\",\"children\":\"Presenter: Sagnik Majumder\"}]]}]]}]]}]]}]}]]}],[\"$\",\"tr\",\"5\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"11:45 – 12:20\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-baseline gap-2 whitespace-nowrap font-semibold text-indigo-800\",\"children\":[[\"$\",\"span\",null,{\"className\":\"rounded-full bg-indigo-600/15 px-2 py-1 text-xs font-bold uppercase tracking-wide text-indigo-700\",\"children\":\"Keynote\"}],\"Addison Lin Wang — 360 Vision in the Foundation AI Era: Principles, Methods, and Future Directions\"]}],false]}]}]]}],[\"$\",\"tr\",\"6\",{\"className\":\"odd:bg-white even:bg-indigo-50/40 align-top\",\"children\":[[\"$\",\"td\",null,{\"className\":\"whitespace-nowrap px-6 py-4 text-lg font-semibold text-indigo-700\",\"children\":\"12:20 – 12:35\"}],[\"$\",\"td\",null,{\"className\":\"px-6 py-4 align-top text-base text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold text-gray-900\",\"children\":\"Awards Ceremony \u0026 Concluding Remarks\"}],false]}]}]]}]]}]]}]}]]}]}],[\"$\",\"section\",null,{\"id\":\"challenge\",\"className\":\"mx-auto mt-24 max-w-6xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"BinEgo‑360° Challenge\"}],[\"$\",\"p\",null,{\"className\":\"mt-6 text-gray-700\",\"children\":[\"The challenge uses our public dataset \",[\"$\",\"a\",null,{\"href\":\"https://x360dataset.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"underline\",\"children\":\"360+x\"}],\" for training/validation, and a held-out test set for the evaluation. \",[\"$\",\"ul\",null,{\"children\":\"Apart from the prizes, winners of the challenge will be invited to submit a paper/report to be included in the workshop proceedings and present at the workshop.\"}],\" For more details about the dataset, tracks, timeline, and submission rules, please see below:\"]}],[\"$\",\"div\",null,{\"className\":\"mt-6 rounded border-l-4 border-yellow-400 bg-yellow-50 p-4 text-gray-700\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-semibold\",\"children\":\"Important Notice:\"}],[\"$\",\"p\",null,{\"className\":\"mt-2\",\"children\":[\"$\",\"b\",null,{\"children\":\"New Final Submission Deadline  ▶  20 August 2025 (AOE)\"}]}],[\"$\",\"p\",null,{\"className\":\"mt-2\",\"children\":\"We are delighted to let you know that the challenge deadline has been extended! Now you have more time to cook your good stuff!\"}]]}]]}],[\"$\",\"section\",null,{\"id\":\"dataset\",\"className\":\"mx-auto mt-24 max-w-6xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Dataset Overview\"}],[\"$\",\"div\",null,{\"className\":\"mt-8 grid gap-8 md:grid-cols-2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full h-[400px] overflow-hidden rounded-lg shadow-md\",\"children\":[\"$\",\"img\",null,{\"src\":\"https://x360dataset.github.io/static/images/overall.gif\",\"alt\":\"Dataset montage\",\"className\":\"w-full h-full object-cover\"}]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center text-gray-700\",\"children\":[[\"$\",\"ul\",null,{\"className\":\"space-y-3\",\"children\":[[\"$\",\"li\",null,{\"children\":\"2,152 videos – 8.579 M frames / 67.78 h.\"}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Viewpoints\"}],\": 360° panoramic, binocular \u0026 monocular egocentric, third‑person front.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Modalities\"}],\": RGB video, 6‑channel spatial audio, GPS + weather, text scene description.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Annotations\"}],\": 38 action classes, temporal segments; object bounding boxes.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Resolution\"}],\": 5 K originals (5 760 × 2 880 pano).\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"License\"}],\": CC BY‑NC‑SA 4.0. All faces auto‑blurred.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-6 flex flex-wrap gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/datasets/quchenyuan/360x_dataset_HR\",\"className\":\"rounded bg-gray-800 px-4 py-2 text-sm font-medium text-white hover:bg-gray-900\",\"children\":\"Download HR\"}],[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/datasets/quchenyuan/360x_dataset_LR\",\"className\":\"rounded border border-gray-300 bg-white px-4 py-2 text-sm font-medium text-gray-700 hover:border-gray-400\",\"children\":\"Download LR\"}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2404.00989\",\"className\":\"rounded border border-indigo-600 bg-indigo-50 px-4 py-2 text-sm font-medium text-indigo-700 hover:bg-indigo-100\",\"children\":\"Paper (CVPR 2024)\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/x360dataset/x360dataset-kit\",\"className\":\"rounded border border-gray-300 bg-white px-4 py-2 text-sm font-medium text-gray-700 hover:border-gray-400\",\"children\":\"Baseline Code\"}]]}]]}]]}]]}],[\"$\",\"section\",null,{\"id\":\"tracks\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Challenge Tracks \u0026 Baselines\"}],[\"$\",\"div\",null,{\"className\":\"mt-10 grid gap-20 md:grid-cols-2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-between rounded-xl border border-gray-200 bg-white p-6 shadow-sm\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-indigo-600\",\"children\":\"1 · Classification\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-gray-600\",\"children\":\"Predict the scene label for a whole clip. We follow the scene categories provided in the dataset.\"}],[\"$\",\"ul\",null,{\"className\":\"mt-4 list-disc space-y-1 pl-5 text-sm text-gray-700\",\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Input\"}],\": 360° RGB + egocentric RGB + audio/binaural delay.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Output\"}],\": The scene label.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Metric\"}],\": Top‑1 Accuracy (in test set).\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-6 rounded bg-gray-50 p-4 text-sm\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium\",\"children\":\"Baseline (All views and modalities use)\"}],[\"$\",\"p\",null,{\"children\":[\"Top‑1 Acc: \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"80.62  %\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-8 text-center\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.kaggle.com/competitions/bin-ego-360-challenge-classification-ext\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-sky-500 px-6 py-3 text-white shadow hover:bg-sky-600 transition\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"children\":[\"$\",\"path\",null,{\"d\":\"M4 4h4v7.586l8.293-8.293 2.414 2.414L10.414 14l8.293 8.293-2.414 2.414L8 16.414V24H4V4z\"}]}],\"Join on Kaggle\"]}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-between rounded-xl border border-gray-200 bg-white p-6 shadow-sm\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-indigo-600\",\"children\":\"2 · Temporal Action Localization\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-gray-600\",\"children\":\"Detect the start and end time of every action instance inside a clip.\"}],[\"$\",\"ul\",null,{\"className\":\"mt-4 list-disc space-y-1 pl-5 text-sm text-gray-700\",\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Input\"}],\": Same modalities as Track 1\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Output\"}],\": JSON output for each detection:\",[\"$\",\"code\",null,{\"children\":\"{\\\"video_id\\\": ..., \\\"t_start\\\": ..., \\\"t_end\\\": ..., \\\"label\\\": ...}\"}]]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Metric\"}],\": mAP averaged over IoU ∈ \",\"{0.5, 0.75, 0.95}\",\".\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-6 rounded bg-gray-50 p-4 text-sm\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium mb-2\",\"children\":\"Baseline (TriDet + VAD)\"}],[\"$\",\"table\",null,{\"className\":\"w-full text-sm text-left text-gray-700 border border-gray-200 rounded\",\"children\":[[\"$\",\"thead\",null,{\"className\":\"bg-gray-100\",\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"className\":\"px-3 py-2 border-b\",\"children\":\"Metric\"}],[\"$\",\"th\",null,{\"className\":\"px-3 py-2 border-b\",\"children\":\"Score\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b\",\"children\":\"mAP@0.5\"}],[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b font-semibold\",\"children\":\"27.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b\",\"children\":\"mAP@0.75\"}],[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b font-semibold\",\"children\":\"18.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b\",\"children\":\"mAP@0.95\"}],[\"$\",\"td\",null,{\"className\":\"px-3 py-2 border-b font-semibold\",\"children\":\"7.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"className\":\"px-3 py-2\",\"children\":\"Average\"}],[\"$\",\"td\",null,{\"className\":\"px-3 py-2 font-semibold\",\"children\":\"17.6\"}]]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-8 text-center\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.kaggle.com/competitions/bin-ego-360-challenge-tal-ext\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-sky-500 px-6 py-3 text-white shadow hover:bg-sky-600 transition\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"children\":[\"$\",\"path\",null,{\"d\":\"M4 4h4v7.586l8.293-8.293 2.414 2.414L10.414 14l8.293 8.293-2.414 2.414L8 16.414V24H4V4z\"}]}],\"Join on Kaggle\"]}]}]]}]]}]]}],[\"$\",\"section\",null,{\"id\":\"timeline\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Timeline (Anywhere on Earth)\"}],[\"$\",\"ol\",null,{\"className\":\"mt-8 border-l-2 border-indigo-600\",\"children\":[[\"$\",\"li\",\"0\",{\"className\":\"relative ml-6 pb-8 last:pb-0\",\"children\":[[\"$\",\"span\",null,{\"className\":\"absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600\"}],[\"$\",\"span\",null,{\"className\":\"font-medium text-gray-900\",\"children\":\"1 Jun 2025\"}],[\"$\",\"span\",null,{\"className\":\"ml-2 text-gray-600\",\"children\":\"Dataset \u0026 baselines release; Kaggle opens\"}]]}],[\"$\",\"li\",\"1\",{\"className\":\"relative ml-6 pb-8 last:pb-0\",\"children\":[[\"$\",\"span\",null,{\"className\":\"absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600\"}],[\"$\",\"span\",null,{\"className\":\"font-medium text-gray-900\",\"children\":\"20 Aug 2025\"}],[\"$\",\"span\",null,{\"className\":\"ml-2 text-gray-600\",\"children\":\"Submission deadline\"}]]}],[\"$\",\"li\",\"2\",{\"className\":\"relative ml-6 pb-8 last:pb-0\",\"children\":[[\"$\",\"span\",null,{\"className\":\"absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600\"}],[\"$\",\"span\",null,{\"className\":\"font-medium text-gray-900\",\"children\":\"Sep 2025\"}],[\"$\",\"span\",null,{\"className\":\"ml-2 text-gray-600\",\"children\":\"Technical report and poster due\"}]]}],[\"$\",\"li\",\"3\",{\"className\":\"relative ml-6 pb-8 last:pb-0\",\"children\":[[\"$\",\"span\",null,{\"className\":\"absolute -left-3 top-1.5 h-2 w-2 rounded-full bg-indigo-600\"}],[\"$\",\"span\",null,{\"className\":\"font-medium text-gray-900\",\"children\":\"19-20 Oct 2025\"}],[\"$\",\"span\",null,{\"className\":\"ml-2 text-gray-600\",\"children\":\"Awards \u0026 talks at ICCV 2025 workshop\"}]]}]]}]]}],[\"$\",\"section\",null,{\"id\":\"submission\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Submission Rules\"}],[\"$\",\"ol\",null,{\"className\":\"mt-6 list-decimal space-y-3 pl-6 text-gray-700\",\"children\":[[\"$\",\"li\",null,{\"children\":\"Teams (≤  5 members) register on Kaggle and fill in the team form.\"}],[\"$\",\"li\",null,{\"children\":[\"Up to \",[\"$\",\"strong\",null,{\"children\":\"5 submissions per track per team\"}],\" – the last one counts.\"]}],[\"$\",\"li\",null,{\"children\":\"The winners need to submit a technical report and a poster to be presented at the workshop\"}],[\"$\",\"li\",null,{\"children\":\"No external data that overlaps with the hidden test clips.\"}],[\"$\",\"li\",null,{\"children\":\"Any submission after the deadline will not be considered.\"}]]}]]}],[\"$\",\"section\",null,{\"id\":\"prizes\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Prizes \u0026 Sponsors\"}],[\"$\",\"ul\",null,{\"className\":\"mt-6 space-y-2 text-gray-700\",\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Hardware:\"}],\" Insta360 X5 panoramic camera (\",[\"$\",\"a\",null,{\"href\":\"https://www.insta360.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Insta360\"}],\")\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute:\"}],\" GPU Cloud Credit (worth £5-15k) (\",[\"$\",\"a\",null,{\"href\":\"https://www.scan.co.uk/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"SCAN\"}],\")\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Gift:\"}],\" Amazon / Taobao vouchers (\",[\"$\",\"a\",null,{\"href\":\"https://www.allsee-tech.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Allsee\"}],\")\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Registration:\"}],\" Workshop fee for the 1st \u0026 2nd winners on each track (\",[\"$\",\"a\",null,{\"href\":\"https://www.tencent.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Tencent\"}],\")\"]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-gray-700\",\"children\":[\"Sponsored by \",[\"$\",\"a\",null,{\"href\":\"https://www.insta360.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Insta360\"}],\" · \",[\"$\",\"a\",null,{\"href\":\"https://www.scan.co.uk/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"SCAN\"}],\" · \",[\"$\",\"a\",null,{\"href\":\"https://www.allsee-tech.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Allsee\"}],\" · \",[\"$\",\"a\",null,{\"href\":\"https://www.tencent.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"Tencent\"}]]}]]}],[\"$\",\"section\",null,{\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Ethics \u0026 Broader Impact\"}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-gray-700\",\"children\":\"All videos were recorded in public or non‑sensitive areas with informed participant consent. Faces are automatically blurred, and the dataset is released for non‑commercial research under CC BY‑NC‑SA 4.0. We prohibit any re‑identification, surveillance or commercial use. By advancing robust multi‑modal perception, we aim to benefit robotics, AR/VR and assistive tech while upholding fairness and privacy.\"}]]}],[\"$\",\"section\",null,{\"id\":\"organizers\",\"className\":\"mx-auto mt-24 max-w-4xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Organisers\"}],[\"$\",\"div\",null,{\"className\":\"mt-8 grid gap-8 md:grid-cols-3 lg:grid-cols-4\",\"children\":[[\"$\",\"div\",\"Jianbo Jiao\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://jianbojiao.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/jiao.jpg\",\"alt\":\"Jianbo Jiao\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://jianbojiao.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Jianbo Jiao\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Birmingham\"}]]}],[\"$\",\"div\",\"Shangzhe Wu\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://elliottwu.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/wu.jpg\",\"alt\":\"Shangzhe Wu\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://elliottwu.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Shangzhe Wu\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Cambridge\"}]]}],[\"$\",\"div\",\"Dylan Campbell\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://sites.google.com/view/djcampbell\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/campbell.jpg\",\"alt\":\"Dylan Campbell\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://sites.google.com/view/djcampbell\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Dylan Campbell\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"Australian National University\"}]]}],[\"$\",\"div\",\"Yunchao Wei\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://weiyc.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/wei.jpg\",\"alt\":\"Yunchao Wei\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://weiyc.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Yunchao Wei\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"Beijing Jiaotong University\"}]]}],[\"$\",\"div\",\"Lu Qi\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"http://luqi.info/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/qi.jpg\",\"alt\":\"Lu Qi\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"http://luqi.info/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Lu Qi\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"Insta360\"}]]}],[\"$\",\"div\",\"Yasmine Mellah\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.audioscenic.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/placeholder_female.jpg\",\"alt\":\"Yasmine Mellah\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.audioscenic.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Yasmine Mellah\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"Audioscenic\"}]]}],[\"$\",\"div\",\"Aleš Leonardis\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/leonardis-ales\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/leonardis.jpg\",\"alt\":\"Aleš Leonardis\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/leonardis-ales\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Aleš Leonardis\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Birmingham\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-gray-700\",\"children\":[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Technical Committee:\"}]}],[\"$\",\"div\",null,{\"className\":\"mt-8 grid gap-8 md:grid-cols-3 lg:grid-cols-4\",\"children\":[[\"$\",\"div\",\"Chenyuan Qu\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://chenyuanqu.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/chenyuan.jpg\",\"alt\":\"Chenyuan Qu\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://chenyuanqu.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Chenyuan Qu\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Birmingham\"}]]}],[\"$\",\"div\",\"Han Hu\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/Publicationss?hl=zh-CN\u0026view_op=list_works\u0026gmla=ALUCkoXDRY1FyBSlDC4q0bpK9zpnxnhaf2PzJqv2dgVESTCALg71TCdFa7PGpFqiTrWvhnZalzAY234KBYkLCs4O7U4\u0026user=UJRtTJ0AAAAJ\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/han.jpg\",\"alt\":\"Han Hu\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/Publicationss?hl=zh-CN\u0026view_op=list_works\u0026gmla=ALUCkoXDRY1FyBSlDC4q0bpK9zpnxnhaf2PzJqv2dgVESTCALg71TCdFa7PGpFqiTrWvhnZalzAY234KBYkLCs4O7U4\u0026user=UJRtTJ0AAAAJ\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Han Hu\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Birmingham\"}]]}],[\"$\",\"div\",\"Qiming Huang\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://qiming-huang.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/qiming.jpg\",\"alt\":\"Qiming Huang\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://qiming-huang.github.io/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Qiming Huang\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Birmingham\"}]]}],[\"$\",\"div\",\"Hao Chen\",{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://h-chen.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/organizers/hao.jpg\",\"alt\":\"Hao Chen\",\"className\":\"mx-auto h-28 w-28 rounded-full object-cover shadow-md hover:shadow-lg\"}]}],[\"$\",\"h3\",null,{\"className\":\"mt-3 text-base font-semibold text-gray-900\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://h-chen.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"className\":\"hover:underline\",\"children\":\"Hao Chen\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600\",\"children\":\"University of Cambridge\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-gray-700\",\"children\":[\"Contact: \",[\"$\",\"a\",null,{\"href\":\"mailto:j.jiao@bham.ac.uk\",\"className\":\"text-indigo-600 hover:underline\",\"children\":\"j.jiao@bham.ac.uk\"}]]}]]}],[\"$\",\"section\",null,{\"id\":\"sponsors\",\"className\":\"mx-auto mt-24 max-w-10xl px-4 text-center\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Sponsors\"}],[\"$\",\"p\",null,{\"className\":\"mt-6 text-gray-700\",\"children\":\"We gratefully acknowledge the generous support of our sponsors.\"}],[\"$\",\"div\",null,{\"className\":\"mt-10 flex flex-wrap justify-center gap-10\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.insta360.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/insta360-logo.png\",\"alt\":\"Insta360\",\"className\":\"h-24 object-contain\"}]}],[\"$\",\"a\",null,{\"href\":\"https://www.scan.co.uk/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/SCAN-logo.png\",\"alt\":\"SCAN\",\"className\":\"h-24 object-contain\"}]}],[\"$\",\"a\",null,{\"href\":\"https://www.allsee-tech.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/allsee-logo.jpg\",\"alt\":\"Allsee\",\"className\":\"h-24 object-contain\"}]}],[\"$\",\"a\",null,{\"href\":\"https://www.tencent.com/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"$\",\"img\",null,{\"src\":\"/BinEgo-360/tencent_logo.png\",\"alt\":\"Tencent\",\"className\":\"h-24 object-contain\"}]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"mx-auto mt-24 max-w-3xl px-4\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-gray-900\",\"children\":\"Publication(s)\"}],[\"$\",\"p\",null,{\"className\":\"mt-4 text-gray-700\",\"children\":\"If you use the 360+x dataset or participate in the challenge, please consider cite:\"}],[\"$\",\"pre\",null,{\"className\":\"mt-4 rounded bg-gray-100 p-4 text-sm leading-tight text-gray-800 overflow-x-auto\",\"children\":\"@inproceedings{chen2024x360,\\n  title     = {360+x: A Panoptic Multi-modal Scene Understanding Dataset},\\n  author    = {Chen, Hao and Hou, Yuqi and Qu, Chenyuan and Testini, Irene and Hong, Xiaohan and Jiao, Jianbo},\\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n  year      = {2024}\\n}\"}]]}],[\"$\",\"footer\",null,{\"className\":\"mt-32 bg-gray-50 py-6 text-center text-sm text-gray-600\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2025,\" BinEgo‑360° Workshop. Built with Next.js \u0026 Tailwind CSS. Hosted on GitHub Pages.\"]}]}]]}]],[\"$\",\"$L4\",null,{\"children\":\"$L5\"}],null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"Dn3whDzG-xjS11hN-dYjR\",{\"children\":[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:\"$Sreact.suspense\"\nf:I[4911,[],\"AsyncMetadata\"]\n5:[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"promise\":\"$@10\"}]}]\n"])</script><script>self.__next_f.push([1,"8:null\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"10:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"BinEgo‑360 Challenge\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"The BinEgo‑360 Challenge targets human‑like perception by jointly reasoning over 360° panoramic and binocular egocentric video streams, aligned with spatial audio, text and geo‑metadata.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/BinEgo-360/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}]],\"error\":null,\"digest\":\"$undefined\"}\na:{\"metadata\":\"$10:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>